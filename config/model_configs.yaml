# Model Configuration
model:
  architecture: ensemble  # Options: transformer, lstm_gru, graph_nn, ensemble
  
  transformer:
    d_model: 256
    n_heads: 8
    n_encoder_layers: 6
    dim_feedforward: 1024
    dropout: 0.1
    activation: gelu
    positional_encoding: temporal
    max_sequence_length: 60
    
  lstm_gru:
    lstm_layers: 2
    lstm_hidden_size: 128
    gru_layers: 1
    gru_hidden_size: 64
    bidirectional: true
    dropout: 0.2
    attention: bahdanau
    
  graph_nn:
    n_graph_layers: 3
    hidden_dim: 128
    output_dim: 64
    aggregation: mean
    activation: relu
    
  ensemble:
    models:
      - transformer
      - lstm_gru
      - graph_nn
    meta_learner: weighted_average
    uncertainty_estimation: monte_carlo_dropout
    n_mc_samples: 20

# Training Configuration
training:
  # Basic settings
  epochs: 100
  batch_size: 32
  validation_split: 0.2
  test_split: 0.2
  random_seed: 42
  
  # Optimizer
  optimizer:
    type: adamw
    learning_rate: 0.001
    weight_decay: 0.00001
    betas: [0.9, 0.999]
    eps: 1e-08
    
  # Scheduler
  scheduler:
    type: cosine_annealing_warm_restarts
    T_0: 10
    T_mult: 2
    eta_min: 0.00001
    
  # Loss function
  loss:
    type: combined
    mse_weight: 0.4
    directional_weight: 0.3
    sharpe_weight: 0.3
    
  # Regularization
  regularization:
    dropout: 0.2
    gradient_clipping: 1.0
    early_stopping_patience: 15
    
  # Advanced techniques
  advanced:
    adversarial_training: true
    adversarial_weight: 0.1
    contrastive_learning: true
    contrastive_weight: 0.1
    mixup_alpha: 0.2
    
  # Validation
  validation:
    method: time_series_split
    n_splits: 5
    gap: 5  # days

# Prediction Configuration
prediction:
  horizon: 5  # days
  target: relative_return  # Options: absolute_return, relative_return, direction
  confidence_intervals: [0.05, 0.25, 0.75, 0.95]
  
# Hardware Configuration
hardware:
  device: cuda  # Options: cuda, cpu, mps
  mixed_precision: true
  num_workers: 4
  pin_memory: true
  
# Logging Configuration
logging:
  level: INFO
  experiment_tracking: wandb  # Options: mlflow, wandb, tensorboard
  checkpoint_dir: ./checkpoints
  log_interval: 10
  save_best_only: true